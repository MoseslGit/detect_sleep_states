{"cells":[{"cell_type":"markdown","metadata":{},"source":["# References\n","\n","## Finding critical points using NNs\n","The idea is basically to focus only on the transition times between wake/sleep, training a custom model (Recurrent + ResNet cpts) to do it\n","\n","model = MultiResidualBiGRU(input_size=10,hidden_size=64,out_size=2,n_layers=5).to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-3,weight_decay = 0)\n","\n","Bidirectional GRU takes inputs in the forward and backwards directions, and we combine with residual layers\n","\n","https://ieeexplore.ieee.org/document/9836276\n","\n","MultiResidualBiGRU combines 5 of these\n","\n","https://www.kaggle.com/code/werus23/sleep-critical-point-infer\n","\n","\n","## Random Forest model starter\n","Trained on a subset of the full series: 35/277, excluding noisy datasets\n","\n","https://www.kaggle.com/code/carlmcbrideellis/zzzs-random-forest-model-starter\n","\n","## Estimating sleep parameters using an accelerometer\n","Using sleep-detection algorithm based off of meeting a rolling median threshold combined with block length to create extra features\n","\n","https://www.nature.com/articles/s41598-018-31266-z\n","\n","## XGBoost Regressor\n","Create a feature measuring how far off the measurement model is from accurate\n","\n","https://analyticsindiamag.com/how-to-use-xgboost-for-time-series-analysis/\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-25T08:31:13.920964Z","iopub.status.busy":"2023-10-25T08:31:13.920588Z","iopub.status.idle":"2023-10-25T08:31:16.630870Z","shell.execute_reply":"2023-10-25T08:31:16.629877Z","shell.execute_reply.started":"2023-10-25T08:31:13.920934Z"},"trusted":true},"outputs":[],"source":["import os\n","from datetime import datetime\n","import random\n","import math\n","\n","import pandas as pd\n","import numpy as np\n","import gc\n","import matplotlib.pyplot as plt\n","from tqdm.auto import tqdm\n","from scipy.interpolate import interp1d\n","from math import pi, sqrt, exp\n","import sklearn\n","import torch\n","from torch import nn, Tensor\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler\n","from sklearn.metrics import average_precision_score\n","from sklearn.ensemble import RandomForestClassifier\n","from itertools import groupby\n","import pickle\n","\n","import pyarrow as pa\n","from pyarrow.parquet import ParquetFile\n","\n","# GPU Configuration\n","import ctypes\n","torch.set_num_interop_threads(4)\n","torch.set_num_threads(4)\n","\n","# Device Selection\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"]},{"cell_type":"markdown","metadata":{},"source":["## Config/Data Loading"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-25T08:31:16.633521Z","iopub.status.busy":"2023-10-25T08:31:16.632772Z","iopub.status.idle":"2023-10-25T08:31:16.655710Z","shell.execute_reply":"2023-10-25T08:31:16.654725Z","shell.execute_reply.started":"2023-10-25T08:31:16.633486Z"},"trusted":true},"outputs":[],"source":["class PATHS:\n","    MAIN_DIR = \"/kaggle/input/child-mind-institute-detect-sleep-states/\"\n","    # CSV FILES : \n","    SUBMISSION = MAIN_DIR + \"sample_submission.csv\"\n","    TRAIN_EVENTS = MAIN_DIR + \"train_events.csv\"\n","    # PARQUET FILES:\n","    TRAIN_SERIES = MAIN_DIR + \"train_series.parquet\"\n","    TEST_SERIES = MAIN_DIR + \"test_series.parquet\"\n","\n","class data_reader:\n","    def __init__(self):\n","        super().__init__()\n","        # MAPPING FOR DATA LOADING :\n","        self.names_mapping = {\n","            \"submission\" : {\"path\" : PATHS.SUBMISSION, \"is_parquet\" : False, \"has_timestamp\" : False}, \n","            \"train_events\" : {\"path\" : PATHS.TRAIN_EVENTS, \"is_parquet\" : False, \"has_timestamp\" : True},\n","            \"train_series\" : {\"path\" : PATHS.TRAIN_SERIES, \"is_parquet\" : True, \"has_timestamp\" : True},\n","            \"test_series\" : {\"path\" : PATHS.TEST_SERIES, \"is_parquet\" : True, \"has_timestamp\" : True}\n","        }\n","        self.valid_names = [\"submission\", \"train_events\", \"train_series\", \"test_series\"]\n","    \n","    def verify(self, data_name):\n","        \"function for data name verification\"\n","        if data_name not in self.valid_names:\n","            print(\"PLEASE ENTER A VALID DATASET NAME, VALID NAMES ARE : \", valid_names)\n","        return\n","    \n","    def cleaning(self, data):\n","        \"cleaning function : drop na values\"\n","        before_cleaning = len(data)\n","        print(\"Number of missing timestamps : \", len(data[data[\"timestamp\"].isna()]))\n","        data = data.dropna(subset=[\"timestamp\"])\n","        after_cleaning = len(data)\n","        print(\"Percentage of removed rows : {:.1f}%\".format(100 * (before_cleaning - after_cleaning) / before_cleaning) )\n","        return data\n","    \n","    @staticmethod\n","    def reduce_memory_usage(data):\n","        \"iterate through all the columns of a dataframe and modify the data type to reduce memory usage.\"\n","        start_mem = data.memory_usage().sum() / 1024**2\n","        print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n","        for col in data.columns:\n","            col_type = data[col].dtype    \n","            if col_type != object:\n","                c_min = data[col].min()\n","                c_max = data[col].max()\n","                if str(col_type)[:3] == 'int':\n","                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n","                        data[col] = data[col].astype(np.int8)\n","                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n","                        data[col] = data[col].astype(np.int16)\n","                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n","                        data[col] = data[col].astype(np.int32)\n","                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n","                        data[col] = data[col].astype(np.int64)  \n","                else:\n","                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n","                        data[col] = data[col].astype(np.float16)\n","                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n","                        data[col] = data[col].astype(np.float32)\n","                    else:\n","                        data[col] = data[col].astype(np.float64)\n","            else:\n","                data[col] = data[col].astype('category')\n","\n","        end_mem = data.memory_usage().sum() / 1024**2\n","        print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n","        print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n","        return data\n","    \n","    def load_data(self, data_name):\n","        \"function for data loading\"\n","        self.verify(data_name)\n","        data_props = self.names_mapping[data_name]\n","        if data_props[\"is_parquet\"]:\n","            data = pd.read_parquet(data_props[\"path\"])\n","        else:\n","            data = pd.read_csv(data_props[\"path\"])\n","                \n","        gc.collect()\n","        if data_props[\"has_timestamp\"]:\n","            print('cleaning')\n","            data = self.cleaning(data)\n","            gc.collect()\n","        data = self.reduce_memory_usage(data)\n","        return data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-25T08:31:16.657747Z","iopub.status.busy":"2023-10-25T08:31:16.657130Z","iopub.status.idle":"2023-10-25T08:31:17.156951Z","shell.execute_reply":"2023-10-25T08:31:17.155783Z","shell.execute_reply.started":"2023-10-25T08:31:16.657717Z"},"trusted":true},"outputs":[],"source":["werus_reader = data_reader()\n","test_series = werus_reader.load_data(data_name=\"test_series\")\n","ids = test_series.series_id.unique()\n","gc.collect()"]},{"cell_type":"markdown","metadata":{},"source":["## Feature Engineering"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def rolling_window_algo(df_group):\n","    \n","    # Steps 3-5\n","    df_group['rolling_median'] = df_group['anglezdiff'].rolling(window=60).median()\n","    # Steps 6-7\n","    df_group['day_id'] = (df_group['timestamp'] - pd.Timedelta(hours=12)).dt.date\n","    thresholds = df_group.groupby('day_id')['rolling_median'].quantile(0.1) * 15\n","    df_group['threshold'] = df_group['day_id'].map(thresholds)\n","    below_threshold_series = (df_group['rolling_median'] < df_group['threshold']).astype('int')\n","    if below_threshold_series.empty or below_threshold_series.sum() == 0:\n","        df_group['rolling_algo_awake'] = 0\n","        df_group.drop(columns=['day_id', 'threshold', 'rolling_median'], inplace=True)\n","        gc.collect()\n","        print(df_group)\n","        return df_group\n","    df_group['below_threshold'] = below_threshold_series\n","    block_diff_series = below_threshold_series.diff()\n","    df_group['block_diff'] = block_diff_series\n","    df_group['block_start'] = (block_diff_series == 1).astype('int')\n","    df_group['block_end'] = (block_diff_series == -1).astype('int')\n","    if below_threshold_series.iloc[0] == 1:\n","        df_group.at[0, 'block_start'] = 1\n","    if below_threshold_series.iloc[-1] == 1:\n","        df_group.at[-1, 'block_end'] = 1\n","    block_start_times = df_group.loc[df_group['block_start'] == 1, 'timestamp'].values\n","    block_end_times = df_group.loc[df_group['block_end'] == 1, 'timestamp'].values\n","    block_durations = block_end_times - block_start_times\n","    valid_block_mask = block_durations > np.timedelta64(30, 'm')\n","    df_group['valid_block'] = 0\n","    df_group.loc[df_group['block_start'] == 1, 'valid_block'] = valid_block_mask.astype(int)\n","    df_group.loc[df_group['block_end'] == 1, 'valid_block'] = valid_block_mask.astype(int)\n","    # Step 8\n","    gap_durations = block_start_times[1:] - block_end_times[:-1]\n","    valid_gap_mask = gap_durations < np.timedelta64(60, 'm')\n","    gap_start_indices = df_group[df_group['block_end'] == 1].index[:-1][valid_gap_mask]\n","    gap_end_indices = df_group[df_group['block_start'] == 1].index[1:][valid_gap_mask]\n","    df_group.loc[gap_start_indices, 'valid_block'] = 1\n","    df_group.loc[gap_end_indices, 'valid_block'] = 1\n","    # Step 9\n","    cum_valid_block = df_group['valid_block'].cumsum()\n","    sleep_period_length = df_group.groupby(['day_id', cum_valid_block])['valid_block'].sum()\n","    longest_block_index = sleep_period_length.idxmax()\n","    df_group['main_sleep_period'] = 0\n","    df_group.loc[(df_group['day_id'] == longest_block_index[0]) & (cum_valid_block == longest_block_index[1]), 'main_sleep_period'] = 1\n","    df_group['rolling_algo_awake'] = (~(df_group['valid_block'] | df_group['main_sleep_period'])).astype(int).fillna(method=\"ffill\")\n","    columns_to_drop = ['day_id', 'threshold', 'below_threshold', 'block_start', 'block_end', 'block_diff', 'valid_block', 'main_sleep_period', 'rolling_median']\n","    df_group.drop(columns=columns_to_drop, inplace=True)\n","    gc.collect()\n","    print(df_group)\n","    return df_group"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-25T08:31:25.224545Z","iopub.status.busy":"2023-10-25T08:31:25.224142Z","iopub.status.idle":"2023-10-25T08:31:25.237896Z","shell.execute_reply":"2023-10-25T08:31:25.237081Z","shell.execute_reply.started":"2023-10-25T08:31:25.224506Z"},"trusted":true},"outputs":[],"source":["def make_features(df):\n","    # parse the timestamp and create an \"hour\" feature\n","    df['series_id'] = df['series_id'].astype('category')\n","    df['timestamp'] = pd.to_datetime(df['timestamp']).apply(lambda t: t.tz_localize(None))\n","    df[\"hour\"] = df[\"timestamp\"].dt.hour\n","    df['minute'] = df['timestamp'].dt.minute\n","    # Use .fillna(value = 0) or .interpolate(method='linear')?\n","    df[\"anglezdiff\"] = df[\"anglez\"].diff().abs().astype(np.float32)\n","\n","    df.sort_values(['timestamp'], inplace=True)\n","\n","    # Rolling window algo\n","\n","    df = df.groupby('series_id').apply(rolling_window_algo).reset_index(drop=True)\n","\n","    new_columns = []\n","        \n","    # periods in seconds        \n","    periods = [60, 360, 720] \n","    for col in ['enmo', 'anglez', 'anglezdiff']:\n","        \n","        for n in periods:\n","            \n","            \n","            rol_args = {'window': int(n/5), 'min_periods':10, 'center':True}\n","            \n","            for agg in ['median', 'mean', 'max', 'min', 'var']:\n","                new_col_name = f'{col}_{agg}_{n}'\n","                new_col = df[col].rolling(**rol_args).agg(agg).astype(np.float32)\n","                new_col.name = new_col_name\n","                new_columns.append(new_col)\n","\n","    df = pd.concat([df] + new_columns, axis=1)\n","\n","    df.drop(columns=['timestamp'], inplace=True)\n","    df.dropna(inplace=True)\n","\n","    return df"]},{"cell_type":"markdown","metadata":{},"source":["## Model Initialization"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-25T08:35:36.725202Z","iopub.status.busy":"2023-10-25T08:35:36.724899Z","iopub.status.idle":"2023-10-25T08:35:36.738274Z","shell.execute_reply":"2023-10-25T08:35:36.737414Z","shell.execute_reply.started":"2023-10-25T08:35:36.725176Z"},"trusted":true},"outputs":[],"source":["class ResidualBiGRU(nn.Module):\n","    def __init__(self, hidden_size, n_layers=1, bidir=True):\n","        super(ResidualBiGRU, self).__init__()\n","\n","        self.hidden_size = hidden_size\n","        self.n_layers = n_layers\n","\n","        self.gru = nn.GRU(\n","            hidden_size,\n","            hidden_size,\n","            n_layers,\n","            batch_first=True,\n","            bidirectional=bidir,\n","        )\n","        dir_factor = 2 if bidir else 1\n","        self.fc1 = nn.Linear(\n","            hidden_size * dir_factor, hidden_size * dir_factor * 2\n","        )\n","        self.ln1 = nn.LayerNorm(hidden_size * dir_factor * 2)\n","        self.fc2 = nn.Linear(hidden_size * dir_factor * 2, hidden_size)\n","        self.ln2 = nn.LayerNorm(hidden_size)\n","\n","    def forward(self, x, h=None):\n","        res, new_h = self.gru(x, h)\n","        # res.shape = (batch_size, sequence_size, 2*hidden_size)\n","\n","        res = self.fc1(res)\n","        res = self.ln1(res)\n","        res = nn.functional.relu(res)\n","\n","        res = self.fc2(res)\n","        res = self.ln2(res)\n","        res = nn.functional.relu(res)\n","\n","        # skip connection\n","        res = res + x\n","\n","        return res, new_h\n","\n","class MultiResidualBiGRU(nn.Module):\n","    def __init__(self, input_size, hidden_size, out_size, n_layers, bidir=True):\n","        super(MultiResidualBiGRU, self).__init__()\n","\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.out_size = out_size\n","        self.n_layers = n_layers\n","\n","        self.fc_in = nn.Linear(input_size, hidden_size)\n","        self.ln = nn.LayerNorm(hidden_size)\n","        self.res_bigrus = nn.ModuleList(\n","            [\n","                ResidualBiGRU(hidden_size, n_layers=1, bidir=bidir)\n","                for _ in range(n_layers)\n","            ]\n","        )\n","        self.fc_out = nn.Linear(hidden_size, out_size)\n","\n","    def forward(self, x, h=None):\n","        # if we are at the beginning of a sequence (no hidden state)\n","        if h is None:\n","            # (re)initialize the hidden state\n","            h = [None for _ in range(self.n_layers)]\n","\n","        x = self.fc_in(x)\n","        x = self.ln(x)\n","        x = nn.functional.relu(x)\n","\n","        new_h = []\n","        for i, res_bigru in enumerate(self.res_bigrus):\n","            x, new_hi = res_bigru(x, h[i])\n","            new_h.append(new_hi)\n","\n","        x = self.fc_out(x)\n","#         x = F.normalize(x,dim=0)\n","        return x, new_h  # log probabilities + hidden states"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-25T08:35:36.739781Z","iopub.status.busy":"2023-10-25T08:35:36.739271Z","iopub.status.idle":"2023-10-25T08:35:36.939453Z","shell.execute_reply":"2023-10-25T08:35:36.938280Z","shell.execute_reply.started":"2023-10-25T08:35:36.739751Z"},"trusted":true},"outputs":[],"source":["class SleepDataset(Dataset):\n","    def __init__(\n","        self,\n","        series_ids,\n","        series,\n","    ):\n","        series_ids = series_ids\n","        series = series.reset_index()\n","        self.data = []\n","        \n","        for viz_id in tqdm(series_ids):\n","            self.data.append(series.loc[(series.series_id==viz_id)].copy().reset_index())\n","            \n","    def downsample_seq_generate_features(self,feat, downsample_factor):\n","        \n","        if len(feat)%12!=0:\n","            feat = np.concatenate([feat,np.zeros(12-((len(feat))%12))+feat[-1]])\n","        feat = np.reshape(feat, (-1,12))\n","        feat_mean = np.mean(feat,1)\n","        feat_std = np.std(feat,1)\n","        feat_median = np.median(feat,1)\n","        feat_max = np.max(feat,1)\n","        feat_min = np.min(feat,1)\n","\n","        return np.dstack([feat_mean,feat_std,feat_median,feat_max,feat_min])[0]\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        X = self.data[index][['anglez','enmo']].values.astype(np.float32)\n","        X = np.concatenate([self.downsample_seq_generate_features(X[:,i],12) for i in range(X.shape[1])],-1)\n","        X = torch.from_numpy(X)\n","        return X\n","werus_test_ds = SleepDataset(test_series.series_id.unique(),test_series)\n","# del werus_test_series\n","# gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-25T08:35:36.941496Z","iopub.status.busy":"2023-10-25T08:35:36.940830Z","iopub.status.idle":"2023-10-25T08:35:36.969521Z","shell.execute_reply":"2023-10-25T08:35:36.968457Z","shell.execute_reply.started":"2023-10-25T08:35:36.941440Z"},"trusted":true},"outputs":[],"source":["max_chunk_size = 24*60*100\n","min_interval = 30\n","model = MultiResidualBiGRU(input_size=10,hidden_size=64,out_size=2,n_layers=5).to(device).eval()\n","model.load_state_dict(torch.load(f'/kaggle/input/werus-rf-ensemble/werus_model.pth',map_location=device))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-25T08:35:36.972895Z","iopub.status.busy":"2023-10-25T08:35:36.972544Z"},"trusted":true},"outputs":[],"source":["# import classifier models\n","classifier = pickle.load(open('/kaggle/input/xgb-pickle-test/xgb_test.pkl', \"rb\"))"]},{"cell_type":"markdown","metadata":{},"source":["## Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Here Werus downsamples to save computation, so we get the dataframe of 12-step data, and will use it to interpolate the predictions back to 1-step data\n","# werus_submission = pd.DataFrame()\n","werus_preds = pd.DataFrame()\n","for i in range(len(werus_test_ds)):\n","    X = werus_test_ds[i].half()\n","    \n","    seq_len = X.shape[0]\n","    h = None\n","    pred = torch.zeros((len(X),2)).half()\n","    for j in range(0, seq_len, max_chunk_size):\n","        y_pred, h = model(X[j: j + max_chunk_size].float(), h)\n","        h = [hi.detach() for hi in h]\n","        pred[j : j + max_chunk_size] = y_pred.detach()\n","        del y_pred;gc.collect()\n","    del h,X;gc.collect()\n","    pred = pred.numpy()\n","    \n","    series_id = ids[i]\n","    \n","    days = len(pred)/(17280/12)\n","    scores0,scores1 = np.zeros(len(pred),dtype=np.float16),np.zeros(len(pred),dtype=np.float16)\n","    for index in range(len(pred)):\n","        if pred[index,0]==max(pred[max(0,index-min_interval):index+min_interval,0]):\n","            scores0[index] = max(pred[max(0,index-min_interval):index+min_interval,0])\n","        if pred[index,1]==max(pred[max(0,index-min_interval):index+min_interval,1]):\n","            scores1[index] = max(pred[max(0,index-min_interval):index+min_interval,1])\n","\n","    steps = werus_test_ds.data[i][['step']].iloc[np.clip(range(len(pred))*12, 0, len(werus_test_ds.data[i])-1)].astype(np.int32)\n","    temp_df = pd.DataFrame()\n","    temp_df['series_id'] = [series_id] * len(steps)\n","    temp_df['step'] = steps['step'].values\n","    temp_df['werus_score_0'] = scores0\n","    temp_df['werus_score_1'] = scores1\n","    # candidates_onset = np.argsort(scores0)[-max(1,round(days)):]\n","    # candidates_wakeup = np.argsort(scores1)[-max(1,round(days)):]\n","    \n","    # onset = werus_test_ds.data[i][['step']].iloc[np.clip(candidates_onset*12,0,len(werus_test_ds.data[i])-1)].astype(np.int32)\n","    # onset['event'] = 'onset'\n","    # onset['series_id'] = series_id\n","    # onset['score']= scores0[candidates_onset]\n","    # wakeup = werus_test_ds.data[i][['step']].iloc[np.clip(candidates_wakeup*12,0,len(werus_test_ds.data[i])-1)].astype(np.int32)\n","    # wakeup['event'] = 'wakeup'\n","    # wakeup['series_id'] = series_id\n","    # wakeup['score']= scores1[candidates_wakeup]\n","    werus_preds = pd.concat([werus_preds,temp_df],axis=0)\n","    # werus_submission = pd.concat([werus_submission,onset,wakeup],axis=0)\n","    # del onset,wakeup,candidates_onset,candidates_wakeup,scores0,scores1,pred,series_id,\n","    del scores0,scores1,pred,series_id,temp_df\n","    gc.collect()\n","del werus_test_ds\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# werus_submission = werus_submission.sort_values(['series_id','step']).reset_index(drop=True)\n","# werus_submission['row_id'] = werus_submission.index.astype(int)\n","# werus_submission['score'] = werus_submission['score'].fillna(werus_submission['score'].mean())\n","# werus_submission = werus_submission[['row_id','series_id','step','event','score']]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_series_counts(df):\n","    return df.groupby('series_id').size()\n","\n","def distribute_series_ids(series_counts):\n","    sorted_series_ids = series_counts.sort_values(ascending=False).index.tolist()\n","    if len(series_counts) > 10:\n","        distributed_ids = [[] for _ in range(10)]\n","        total_rows = [0] * 10\n","    else:\n","        distributed_ids = [[] for _ in range(len(series_counts))]\n","        total_rows = [0] * len(series_counts)\n","    for series_id in sorted_series_ids:\n","        idx = np.argmin(total_rows)\n","        distributed_ids[idx].append(series_id)\n","        total_rows[idx] += series_counts[series_id]\n","    return distributed_ids"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["gb_result_df = pd.DataFrame(columns=[\"series_id\", \"step\", \"not_awake\", \"awake\"])\n","\n","# Distribute series_ids into 10 subsets\n","series_counts = get_series_counts(test_series)\n","distributed_ids = distribute_series_ids(series_counts)\n","\n","# Iterate through each subset of series_ids, process data and run inference\n","for i, series_id_list in enumerate(distributed_ids):\n","    # Load data for the current subset of series_ids\n","    chunk_df = test_series[test_series['series_id'].isin(series_id_list)]\n","    \n","    # Perform feature engineering\n","    chunk_df = make_features(chunk_df)\n","     # Run inference\n","    chunk_df[\"not_awake\"] = classifier.predict_proba(chunk_df.loc[:, chunk_df.columns != \"series_id\"])[:,0]\n","    chunk_df[\"awake\"] = classifier.predict_proba(chunk_df.loc[:, ~chunk_df.columns.isin([\"series_id\", \"not_awake\"])])[:,1]\n","    \n","    # Append results to gb_result_df\n","    result_chunk = chunk_df[[\"series_id\", \"step\", \"not_awake\", \"awake\"]]\n","    gb_result_df = pd.concat([gb_result_df, result_chunk], ignore_index=True)\n","    del chunk_df\n","    gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test = gb_result_df\n","del gb_result_df\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# test  = make_features(test_series)\n","\n","# del test_series\n","# gc.collect()\n","\n","# test[\"not_awake\"] = classifier.predict_proba(test.loc[:, test.colums != \"series_id\"])[:,0]\n","# test[\"awake\"]     = classifier.predict_proba(test.loc[:, test.colums != \"series_id\"])[:,1]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# test = test[['series_id','step','not_awake','awake', 'rolling_algo_awake', 'anglezdiff', 'enmo', 'enmo_mean_60', 'enmo_var_60', 'minute', 'hour']]\n","# gc.collect()"]},{"cell_type":"markdown","metadata":{},"source":["## Ensembling"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Make a final ensemble df with all the predictions\n","ensemble_df = test.merge(werus_preds, on=['series_id', 'step'], how='left')\n","del test,werus_preds\n","gc.collect()\n","# Interpolate missing values in \"score0\" and \"score1\" columns\n","ensemble_df['werus_score_0'] = ensemble_df['werus_score_0'].interpolate(method='linear')\n","ensemble_df['werus_score_1'] = ensemble_df['werus_score_1'].interpolate(method='linear')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ensemble_df['not_awake'] = ensemble_df['not_awake'].interpolate(method='linear')\n","ensemble_df['awake'] = ensemble_classifier.predict_proba(ensemble_df.loc[:, ensemble_df.columns != \"series_id\"])[:,1]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# smoothing the predictions\n","smoothing_length = 2*230\n","test[\"score\"]  = test[\"awake\"].rolling(smoothing_length,center=True).mean().fillna(method=\"bfill\").fillna(method=\"ffill\")\n","test[\"smooth\"] = test[\"not_awake\"].rolling(smoothing_length,center=True).mean().fillna(method=\"bfill\").fillna(method=\"ffill\")\n","# re-binarize\n","test[\"smooth\"] = test[\"smooth\"].round()\n","\n","# https://stackoverflow.com/questions/73777727/how-to-mark-start-end-of-a-series-of-non-null-and-non-0-values-in-a-column-of-a\n","def get_event(df):\n","    lstCV = zip(df.series_id, df.smooth)\n","    lstPOI = []\n","    for (c, v), g in groupby(lstCV, lambda cv: \n","                            (cv[0], cv[1]!=0 and not pd.isnull(cv[1]))):\n","        llg = sum(1 for item in g)\n","        if v is False: \n","            lstPOI.extend([0]*llg)\n","        else: \n","            lstPOI.extend(['onset']+(llg-2)*[0]+['wakeup'] if llg > 1 else [0])\n","    return lstPOI\n","\n","test[\"event\"] = get_event(test)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["rf_submission = test.loc[test[\"event\"] != 0][[\"series_id\",\"step\",\"event\",\"score\"]].copy().reset_index(drop=True).reset_index(names=\"row_id\")\n","del test   \n","gc.collect()\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":4}
