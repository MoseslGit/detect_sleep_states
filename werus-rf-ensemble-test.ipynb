{"cells":[{"cell_type":"markdown","metadata":{},"source":["# References\n","\n","## Finding critical points using NNs\n","The idea is basically to focus only on the transition times between wake/sleep, training a ResNet to do it\n","\n","model = MultiResidualBiGRU(input_size=10,hidden_size=64,out_size=2,n_layers=5).to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-3,weight_decay = 0)\n","\n","Bidirectional GRU takes inputs in the forward and backwards directions, and we combine with residual layers\n","\n","https://ieeexplore.ieee.org/document/9836276\n","\n","MultiResidualBiGRU combines 5 of these\n","\n","https://www.kaggle.com/code/werus23/sleep-critical-point-infer\n","\n","\n","## Random Forest model starter\n","Trained on a subset of the full series: 35/277, excluding noisy datasets\n","\n","https://www.kaggle.com/code/carlmcbrideellis/zzzs-random-forest-model-starter\n","\n","## Estimating sleep parameters using an accelerometer\n","Using sleep-detection algorithm based off of meeting a rolling median threshold combined with block length to create extra features\n","\n","https://www.nature.com/articles/s41598-018-31266-z\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-25T08:31:13.920964Z","iopub.status.busy":"2023-10-25T08:31:13.920588Z","iopub.status.idle":"2023-10-25T08:31:16.630870Z","shell.execute_reply":"2023-10-25T08:31:16.629877Z","shell.execute_reply.started":"2023-10-25T08:31:13.920934Z"},"trusted":true},"outputs":[],"source":["import os\n","from datetime import datetime\n","import random\n","import math\n","\n","import pandas as pd\n","import numpy as np\n","import gc\n","import matplotlib.pyplot as plt\n","from tqdm.auto import tqdm\n","from scipy.interpolate import interp1d\n","from math import pi, sqrt, exp\n","import sklearn\n","import torch\n","from torch import nn, Tensor\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler\n","from sklearn.metrics import average_precision_score\n","from sklearn.ensemble import RandomForestClassifier\n","from itertools import groupby\n","\n","import pyarrow as pa\n","from pyarrow.parquet import ParquetFile\n","\n","# GPU Configuration\n","import ctypes\n","torch.set_num_interop_threads(4)\n","torch.set_num_threads(4)\n","\n","# Device Selection\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"]},{"cell_type":"markdown","metadata":{},"source":["## Config/Data Loading"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-25T08:31:16.633521Z","iopub.status.busy":"2023-10-25T08:31:16.632772Z","iopub.status.idle":"2023-10-25T08:31:16.655710Z","shell.execute_reply":"2023-10-25T08:31:16.654725Z","shell.execute_reply.started":"2023-10-25T08:31:16.633486Z"},"trusted":true},"outputs":[],"source":["class PATHS:\n","    MAIN_DIR = \"/kaggle/input/child-mind-institute-detect-sleep-states/\"\n","    # CSV FILES : \n","    SUBMISSION = MAIN_DIR + \"sample_submission.csv\"\n","    TRAIN_EVENTS = MAIN_DIR + \"train_events.csv\"\n","    # PARQUET FILES:\n","    TRAIN_SERIES = MAIN_DIR + \"train_series.parquet\"\n","    TEST_SERIES = MAIN_DIR + \"test_series.parquet\"\n","\n","class data_reader:\n","    def __init__(self):\n","        super().__init__()\n","        # MAPPING FOR DATA LOADING :\n","        self.names_mapping = {\n","            \"submission\" : {\"path\" : PATHS.SUBMISSION, \"is_parquet\" : False, \"has_timestamp\" : False}, \n","            \"train_events\" : {\"path\" : PATHS.TRAIN_EVENTS, \"is_parquet\" : False, \"has_timestamp\" : True},\n","            \"train_series\" : {\"path\" : PATHS.TRAIN_SERIES, \"is_parquet\" : True, \"has_timestamp\" : True},\n","            \"test_series\" : {\"path\" : PATHS.TEST_SERIES, \"is_parquet\" : True, \"has_timestamp\" : True}\n","        }\n","        self.valid_names = [\"submission\", \"train_events\", \"train_series\", \"test_series\"]\n","    \n","    def verify(self, data_name):\n","        \"function for data name verification\"\n","        if data_name not in self.valid_names:\n","            print(\"PLEASE ENTER A VALID DATASET NAME, VALID NAMES ARE : \", valid_names)\n","        return\n","    \n","    def cleaning(self, data):\n","        \"cleaning function : drop na values\"\n","        before_cleaning = len(data)\n","        print(\"Number of missing timestamps : \", len(data[data[\"timestamp\"].isna()]))\n","        data = data.dropna(subset=[\"timestamp\"])\n","        after_cleaning = len(data)\n","        print(\"Percentage of removed rows : {:.1f}%\".format(100 * (before_cleaning - after_cleaning) / before_cleaning) )\n","        return data\n","    \n","    @staticmethod\n","    def reduce_memory_usage(data):\n","        \"iterate through all the columns of a dataframe and modify the data type to reduce memory usage.\"\n","        start_mem = data.memory_usage().sum() / 1024**2\n","        print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n","        for col in data.columns:\n","            col_type = data[col].dtype    \n","            if col_type != object:\n","                c_min = data[col].min()\n","                c_max = data[col].max()\n","                if str(col_type)[:3] == 'int':\n","                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n","                        data[col] = data[col].astype(np.int8)\n","                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n","                        data[col] = data[col].astype(np.int16)\n","                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n","                        data[col] = data[col].astype(np.int32)\n","                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n","                        data[col] = data[col].astype(np.int64)  \n","                else:\n","                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n","                        data[col] = data[col].astype(np.float16)\n","                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n","                        data[col] = data[col].astype(np.float32)\n","                    else:\n","                        data[col] = data[col].astype(np.float64)\n","            else:\n","                data[col] = data[col].astype('category')\n","\n","        end_mem = data.memory_usage().sum() / 1024**2\n","        print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n","        print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n","        return data\n","    \n","    def load_data(self, data_name):\n","        \"function for data loading\"\n","        self.verify(data_name)\n","        data_props = self.names_mapping[data_name]\n","        if data_props[\"is_parquet\"]:\n","            data = pd.read_parquet(data_props[\"path\"])\n","        else:\n","            data = pd.read_csv(data_props[\"path\"])\n","                \n","        gc.collect()\n","        if data_props[\"has_timestamp\"]:\n","            print('cleaning')\n","            data = self.cleaning(data)\n","            gc.collect()\n","        data = self.reduce_memory_usage(data)\n","        return data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-25T08:31:16.657747Z","iopub.status.busy":"2023-10-25T08:31:16.657130Z","iopub.status.idle":"2023-10-25T08:31:17.156951Z","shell.execute_reply":"2023-10-25T08:31:17.155783Z","shell.execute_reply.started":"2023-10-25T08:31:16.657717Z"},"trusted":true},"outputs":[],"source":["werus_reader = data_reader()\n","werus_test_series = werus_reader.load_data(data_name=\"test_series\")\n","ids = werus_test_series.series_id.unique()\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-25T08:31:17.160608Z","iopub.status.busy":"2023-10-25T08:31:17.160172Z","iopub.status.idle":"2023-10-25T08:31:25.222745Z","shell.execute_reply":"2023-10-25T08:31:25.221913Z","shell.execute_reply.started":"2023-10-25T08:31:17.160568Z"},"trusted":true},"outputs":[],"source":["train = pd.read_parquet(\"/kaggle/input/werus-rf-ensemble/Zzzs_train_multi.parquet\")"]},{"cell_type":"markdown","metadata":{},"source":["## Feature Engineering"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def rolling_window_algo(df_group):\n","    \n","    # Steps 3-5\n","    df_group[\"anglez_diff\"] = df_group['anglez'].diff().abs().fillna(method=\"bfill\").astype('float16')\n","    df_group['rolling_median'] = df_group['anglez_diff'].rolling(window=60).median()\n","    # Steps 6-7\n","    df_group['day_id'] = (df_group['timestamp'] - pd.Timedelta(hours=12)).dt.date\n","    thresholds = df_group.groupby('day_id')['rolling_median'].quantile(0.1) * 15\n","    df_group['threshold'] = df_group['day_id'].map(thresholds)\n","    df_group['below_threshold'] = (df_group['rolling_median'] < df_group['threshold']).astype(int)\n","    df_group['block_diff'] = df_group['below_threshold'].diff()\n","    df_group['block_start'] = (df_group['block_diff'] == 1).astype(int)\n","    df_group['block_end'] = (df_group['block_diff'] == -1).astype(int)\n","    if df_group['below_threshold'].iloc[0] == 1:\n","        df_group.at[0, 'block_start'] = 1\n","    if df_group['below_threshold'].iloc[-1] == 1:\n","        df_group.at[-1, 'block_end'] = 1\n","    block_start_times = df_group.loc[df_group['block_start'] == 1, 'timestamp'].values\n","    block_end_times = df_group.loc[df_group['block_end'] == 1, 'timestamp'].values\n","    block_durations = block_end_times - block_start_times\n","    valid_block_mask = block_durations > np.timedelta64(30, 'm')\n","    df_group['valid_block'] = 0\n","    df_group.loc[df_group['block_start'] == 1, 'valid_block'] = valid_block_mask.astype(int)\n","    df_group.loc[df_group['block_end'] == 1, 'valid_block'] = valid_block_mask.astype(int)\n","    # Step 8\n","    gap_durations = block_start_times[1:] - block_end_times[:-1]\n","    valid_gap_mask = gap_durations < np.timedelta64(60, 'm')\n","    gap_start_indices = df_group[df_group['block_end'] == 1].index[:-1][valid_gap_mask]\n","    gap_end_indices = df_group[df_group['block_start'] == 1].index[1:][valid_gap_mask]\n","    df_group.loc[gap_start_indices, 'valid_block'] = 1\n","    df_group.loc[gap_end_indices, 'valid_block'] = 1\n","    # Step 9\n","    cum_valid_block = df_group['valid_block'].cumsum()\n","    sleep_period_length = df_group.groupby(['day_id', cum_valid_block])['valid_block'].sum()\n","    longest_block_index = sleep_period_length.idxmax()\n","    df_group['main_sleep_period'] = 0\n","    df_group.loc[(df_group['day_id'] == longest_block_index[0]) & (cum_valid_block == longest_block_index[1]), 'main_sleep_period'] = 1\n","    df_group['rolling_algo_awake'] = (~(df_group['valid_block'] | df_group['main_sleep_period'])).astype(int).fillna(method=\"ffill\")\n","    return df_group\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-25T08:31:25.224545Z","iopub.status.busy":"2023-10-25T08:31:25.224142Z","iopub.status.idle":"2023-10-25T08:31:25.237896Z","shell.execute_reply":"2023-10-25T08:31:25.237081Z","shell.execute_reply.started":"2023-10-25T08:31:25.224506Z"},"trusted":true},"outputs":[],"source":["def make_features(df):\n","    # parse the timestamp and create an \"hour\" feature\n","    df['timestamp'] = pd.to_datetime(df['timestamp']).apply(lambda t: t.tz_localize(None))\n","    df[\"hour\"] = df[\"timestamp\"].dt.hour\n","        \n","    \n","    # Rolling window algo\n","    result_dfs = []\n","    for _, group in df.groupby('series_id'):\n","        processed_group = rolling_window_algo(group)\n","        result_dfs.append(processed_group)\n","\n","    algo_df = pd.concat(result_dfs)\n","    df = algo_df.dropna(axis = 0, subset=['hour'])\n","    del algo_df;gc.collect()\n","    \n","\n","    periods = 10\n","    df[\"anglez_diff\"] = df.groupby('series_id')['anglez'].diff(periods).abs().fillna(method=\"bfill\").astype('float16')\n","    df[\"anglez\"] = abs(df[\"anglez\"])\n","    df[\"enmo_diff\"] = df.groupby('series_id')['enmo'].diff(periods=periods).fillna(method=\"bfill\").astype('float16')\n","    df[\"anglez_rolling_mean\"] = df[\"anglez\"].rolling(periods,center=True).mean().fillna(method=\"bfill\").fillna(method=\"ffill\").astype('float16')\n","    df[\"enmo_rolling_mean\"] = df[\"enmo\"].rolling(periods,center=True).mean().fillna(method=\"bfill\").fillna(method=\"ffill\").astype('float16')\n","    df[\"anglez_rolling_max\"] = df[\"anglez\"].rolling(periods,center=True).max().fillna(method=\"bfill\").fillna(method=\"ffill\").astype('float16')\n","    df[\"enmo_rolling_max\"] = df[\"enmo\"].rolling(periods,center=True).max().fillna(method=\"bfill\").fillna(method=\"ffill\").astype('float16')\n","    df[\"anglez_rolling_std\"] = df[\"anglez\"].rolling(periods,center=True).std().fillna(method=\"bfill\").fillna(method=\"ffill\").astype('float16')\n","    df[\"enmo_rolling_std\"] = df[\"enmo\"].rolling(periods,center=True).std().fillna(method=\"bfill\").fillna(method=\"ffill\").astype('float16')\n","    df[\"anglez_diff_rolling_mean\"] = df[\"anglez_diff\"].rolling(periods,center=True).mean().fillna(method=\"bfill\").fillna(method=\"ffill\").astype('float16')\n","    df[\"enmo_diff_rolling_mean\"] = df[\"enmo_diff\"].rolling(periods,center=True).mean().fillna(method=\"bfill\").fillna(method=\"ffill\").astype('float16')\n","    df[\"anglez_diff_rolling_max\"] = df[\"anglez_diff\"].rolling(periods,center=True).max().fillna(method=\"bfill\").fillna(method=\"ffill\").astype('float16')\n","    df[\"enmo_diff_rolling_max\"] = df[\"enmo_diff\"].rolling(periods,center=True).max().fillna(method=\"bfill\").fillna(method=\"ffill\").astype('float16')\n","    \n","    return df\n","\n","features = [\"hour\",\n","            \"anglez\",\n","            \"anglez_rolling_mean\",\n","            \"anglez_rolling_max\",\n","            \"anglez_rolling_std\",\n","            \"anglez_diff\",\n","            \"anglez_diff_rolling_mean\",\n","            \"anglez_diff_rolling_max\",\n","            \"enmo\",\n","            \"enmo_rolling_mean\",\n","            \"enmo_rolling_max\",\n","            \"enmo_rolling_std\",\n","            \"enmo_diff\",\n","            \"enmo_diff_rolling_mean\",\n","            \"enmo_diff_rolling_max\",\n","            \"rolling_algo_awake\",\n","           ]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-25T08:31:25.240079Z","iopub.status.busy":"2023-10-25T08:31:25.239478Z","iopub.status.idle":"2023-10-25T08:35:36.723689Z","shell.execute_reply":"2023-10-25T08:35:36.722570Z","shell.execute_reply.started":"2023-10-25T08:31:25.240038Z"},"trusted":true},"outputs":[],"source":["train   = make_features(train)\n","\n","X_train = train[features]\n","y_train = train[\"awake\"]\n","\n","del train\n","gc.collect()"]},{"cell_type":"markdown","metadata":{},"source":["## Model Initialization"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-25T08:35:36.725202Z","iopub.status.busy":"2023-10-25T08:35:36.724899Z","iopub.status.idle":"2023-10-25T08:35:36.738274Z","shell.execute_reply":"2023-10-25T08:35:36.737414Z","shell.execute_reply.started":"2023-10-25T08:35:36.725176Z"},"trusted":true},"outputs":[],"source":["class ResidualBiGRU(nn.Module):\n","    def __init__(self, hidden_size, n_layers=1, bidir=True):\n","        super(ResidualBiGRU, self).__init__()\n","\n","        self.hidden_size = hidden_size\n","        self.n_layers = n_layers\n","\n","        self.gru = nn.GRU(\n","            hidden_size,\n","            hidden_size,\n","            n_layers,\n","            batch_first=True,\n","            bidirectional=bidir,\n","        )\n","        dir_factor = 2 if bidir else 1\n","        self.fc1 = nn.Linear(\n","            hidden_size * dir_factor, hidden_size * dir_factor * 2\n","        )\n","        self.ln1 = nn.LayerNorm(hidden_size * dir_factor * 2)\n","        self.fc2 = nn.Linear(hidden_size * dir_factor * 2, hidden_size)\n","        self.ln2 = nn.LayerNorm(hidden_size)\n","\n","    def forward(self, x, h=None):\n","        res, new_h = self.gru(x, h)\n","        # res.shape = (batch_size, sequence_size, 2*hidden_size)\n","\n","        res = self.fc1(res)\n","        res = self.ln1(res)\n","        res = nn.functional.relu(res)\n","\n","        res = self.fc2(res)\n","        res = self.ln2(res)\n","        res = nn.functional.relu(res)\n","\n","        # skip connection\n","        res = res + x\n","\n","        return res, new_h\n","\n","class MultiResidualBiGRU(nn.Module):\n","    def __init__(self, input_size, hidden_size, out_size, n_layers, bidir=True):\n","        super(MultiResidualBiGRU, self).__init__()\n","\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.out_size = out_size\n","        self.n_layers = n_layers\n","\n","        self.fc_in = nn.Linear(input_size, hidden_size)\n","        self.ln = nn.LayerNorm(hidden_size)\n","        self.res_bigrus = nn.ModuleList(\n","            [\n","                ResidualBiGRU(hidden_size, n_layers=1, bidir=bidir)\n","                for _ in range(n_layers)\n","            ]\n","        )\n","        self.fc_out = nn.Linear(hidden_size, out_size)\n","\n","    def forward(self, x, h=None):\n","        # if we are at the beginning of a sequence (no hidden state)\n","        if h is None:\n","            # (re)initialize the hidden state\n","            h = [None for _ in range(self.n_layers)]\n","\n","        x = self.fc_in(x)\n","        x = self.ln(x)\n","        x = nn.functional.relu(x)\n","\n","        new_h = []\n","        for i, res_bigru in enumerate(self.res_bigrus):\n","            x, new_hi = res_bigru(x, h[i])\n","            new_h.append(new_hi)\n","\n","        x = self.fc_out(x)\n","#         x = F.normalize(x,dim=0)\n","        return x, new_h  # log probabilities + hidden states"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-25T08:35:36.739781Z","iopub.status.busy":"2023-10-25T08:35:36.739271Z","iopub.status.idle":"2023-10-25T08:35:36.939453Z","shell.execute_reply":"2023-10-25T08:35:36.938280Z","shell.execute_reply.started":"2023-10-25T08:35:36.739751Z"},"trusted":true},"outputs":[],"source":["class SleepDataset(Dataset):\n","    def __init__(\n","        self,\n","        series_ids,\n","        series,\n","    ):\n","        series_ids = series_ids\n","        series = series.reset_index()\n","        self.data = []\n","        \n","        for viz_id in tqdm(series_ids):\n","            self.data.append(series.loc[(series.series_id==viz_id)].copy().reset_index())\n","            \n","    def downsample_seq_generate_features(self,feat, downsample_factor):\n","        \n","        if len(feat)%12!=0:\n","            feat = np.concatenate([feat,np.zeros(12-((len(feat))%12))+feat[-1]])\n","        feat = np.reshape(feat, (-1,12))\n","        feat_mean = np.mean(feat,1)\n","        feat_std = np.std(feat,1)\n","        feat_median = np.median(feat,1)\n","        feat_max = np.max(feat,1)\n","        feat_min = np.min(feat,1)\n","\n","        return np.dstack([feat_mean,feat_std,feat_median,feat_max,feat_min])[0]\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        X = self.data[index][['anglez','enmo']].values.astype(np.float32)\n","        X = np.concatenate([self.downsample_seq_generate_features(X[:,i],12) for i in range(X.shape[1])],-1)\n","        X = torch.from_numpy(X)\n","        return X\n","werus_test_ds = SleepDataset(werus_test_series.series_id.unique(),werus_test_series)\n","del werus_test_series\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-25T08:35:36.941496Z","iopub.status.busy":"2023-10-25T08:35:36.940830Z","iopub.status.idle":"2023-10-25T08:35:36.969521Z","shell.execute_reply":"2023-10-25T08:35:36.968457Z","shell.execute_reply.started":"2023-10-25T08:35:36.941440Z"},"trusted":true},"outputs":[],"source":["max_chunk_size = 24*60*100\n","min_interval = 30\n","model = MultiResidualBiGRU(input_size=10,hidden_size=64,out_size=2,n_layers=5).to(device).eval()\n","model.load_state_dict(torch.load(f'/kaggle/input/werus-rf-ensemble/werus_model.pth',map_location=device))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-25T08:35:36.972895Z","iopub.status.busy":"2023-10-25T08:35:36.972544Z"},"trusted":true},"outputs":[],"source":["# Random Forest\n","classifier = RandomForestClassifier(n_estimators=50,\n","                                    min_samples_leaf=300,\n","                                    random_state=42,n_jobs=-1)\n","\n","classifier.fit(X_train, y_train)\n","\n","del X_train, y_train\n","gc.collect()"]},{"cell_type":"markdown","metadata":{},"source":["## Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["werus_submission = pd.DataFrame()\n","for i in range(len(werus_test_ds)):\n","    X = werus_test_ds[i].half()\n","    \n","    seq_len = X.shape[0]\n","    h = None\n","    pred = torch.zeros((len(X),2)).half()\n","    for j in range(0, seq_len, max_chunk_size):\n","        y_pred, h = model(X[j: j + max_chunk_size].float(), h)\n","        h = [hi.detach() for hi in h]\n","        pred[j : j + max_chunk_size] = y_pred.detach()\n","        del y_pred;gc.collect()\n","    del h,X;gc.collect()\n","    pred = pred.numpy()\n","    \n","    series_id = ids[i]\n","    \n","    days = len(pred)/(17280/12)\n","    scores0,scores1 = np.zeros(len(pred),dtype=np.float16),np.zeros(len(pred),dtype=np.float16)\n","    for index in range(len(pred)):\n","        if pred[index,0]==max(pred[max(0,index-min_interval):index+min_interval,0]):\n","            scores0[index] = max(pred[max(0,index-min_interval):index+min_interval,0])\n","        if pred[index,1]==max(pred[max(0,index-min_interval):index+min_interval,1]):\n","            scores1[index] = max(pred[max(0,index-min_interval):index+min_interval,1])\n","    candidates_onset = np.argsort(scores0)[-max(1,round(days)):]\n","    candidates_wakeup = np.argsort(scores1)[-max(1,round(days)):]\n","    \n","    onset = werus_test_ds.data[i][['step']].iloc[np.clip(candidates_onset*12,0,len(werus_test_ds.data[i])-1)].astype(np.int32)\n","    onset['event'] = 'onset'\n","    onset['series_id'] = series_id\n","    onset['score']= scores0[candidates_onset]\n","    wakeup = werus_test_ds.data[i][['step']].iloc[np.clip(candidates_wakeup*12,0,len(werus_test_ds.data[i])-1)].astype(np.int32)\n","    wakeup['event'] = 'wakeup'\n","    wakeup['series_id'] = series_id\n","    wakeup['score']= scores1[candidates_wakeup]\n","    werus_submission = pd.concat([werus_submission,onset,wakeup],axis=0)\n","    del onset,wakeup,candidates_onset,candidates_wakeup,scores0,scores1,pred,series_id,\n","    gc.collect()\n","print(werus_submission)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["werus_submission = werus_submission.sort_values(['series_id','step']).reset_index(drop=True)\n","werus_submission['row_id'] = werus_submission.index.astype(int)\n","werus_submission['score'] = werus_submission['score'].fillna(werus_submission['score'].mean())\n","werus_submission = werus_submission[['row_id','series_id','step','event','score']]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test  = pd.read_parquet(\"/kaggle/input/child-mind-institute-detect-sleep-states/test_series.parquet\")\n","\n","test  = make_features(test)\n","\n","X_test = test[features]\n","\n","test[\"not_awake\"] = classifier.predict_proba(X_test)[:,0]\n","test[\"awake\"]     = classifier.predict_proba(X_test)[:,1]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# smoothing the predictions\n","smoothing_length = 2*230\n","test[\"score\"]  = test[\"awake\"].rolling(smoothing_length,center=True).mean().fillna(method=\"bfill\").fillna(method=\"ffill\")\n","test[\"smooth\"] = test[\"not_awake\"].rolling(smoothing_length,center=True).mean().fillna(method=\"bfill\").fillna(method=\"ffill\")\n","# re-binarize\n","test[\"smooth\"] = test[\"smooth\"].round()\n","\n","# https://stackoverflow.com/questions/73777727/how-to-mark-start-end-of-a-series-of-non-null-and-non-0-values-in-a-column-of-a\n","def get_event(df):\n","    lstCV = zip(df.series_id, df.smooth)\n","    lstPOI = []\n","    for (c, v), g in groupby(lstCV, lambda cv: \n","                            (cv[0], cv[1]!=0 and not pd.isnull(cv[1]))):\n","        llg = sum(1 for item in g)\n","        if v is False: \n","            lstPOI.extend([0]*llg)\n","        else: \n","            lstPOI.extend(['onset']+(llg-2)*[0]+['wakeup'] if llg > 1 else [0])\n","    return lstPOI\n","\n","test[\"event\"] = get_event(test)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["rf_submission = test.loc[test[\"event\"] != 0][[\"series_id\",\"step\",\"event\",\"score\"]].copy().reset_index(drop=True).reset_index(names=\"row_id\")"]},{"cell_type":"markdown","metadata":{},"source":["## Ensembling"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df_cross = werus_submission.merge(rf_submission, on=[\"series_id\", \"event\"], suffixes=(\"_left\", \"_right\"))\n","df_cross = df_cross[(df_cross[\"step_left\"] - df_cross[\"step_right\"]).abs() <= 10]\n","df_cross = df_cross.sort_values(by=[\"row_id_left\", \"step_right\"]).drop_duplicates(subset=\"row_id_left\", keep=\"first\")\n","df_cross[\"step_avg\"] = 0.8 * df_cross[\"step_left\"] + 0.2 * df_cross[\"step_right\"]\n","df_cross = df_cross[[\"row_id_left\", \"series_id\", \"step_avg\", \"event\", \"score_left\"]]\n","df_cross.columns = [\"row_id\", \"series_id\", \"step\", \"event\", \"score\"]\n","df_combined = pd.concat([werus_submission[~werus_submission[\"row_id\"].isin(df_cross[\"row_id\"])], df_cross]).sort_values(by=\"row_id\").reset_index(drop=True)\n","df_combined.to_csv(\"submission.csv\", index=False)\n","\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":4}
